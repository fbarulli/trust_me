{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlECxOgVXLst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b15cfb-0cbe-4478-ce3c-88715f70ce7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting ray\n",
            "  Downloading ray-2.41.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting gpyopt\n",
            "  Downloading GPyOpt-1.2.6.tar.gz (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torch-ema\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl.metadata (415 bytes)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray) (1.1.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray) (4.25.6)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray) (2.32.3)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting GPy>=1.8 (from gpyopt)\n",
            "  Downloading GPy-1.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from GPy>=1.8->gpyopt) (1.17.0)\n",
            "Collecting paramz>=0.9.6 (from GPy>=1.8->gpyopt)\n",
            "  Downloading paramz-0.9.6-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.11/dist-packages (from GPy>=1.8->gpyopt) (3.0.11)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.22.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.11/dist-packages (from paramz>=0.9.6->GPy>=1.8->gpyopt) (4.4.2)\n",
            "Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.41.0-cp311-cp311-manylinux2014_x86_64.whl (67.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.2.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Downloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GPy-1.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading paramz-0.9.6-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gpyopt\n",
            "  Building wheel for gpyopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpyopt: filename=GPyOpt-1.2.6-py3-none-any.whl size=83602 sha256=a9874b47146c1fefa62f2bf78a3f995e0596e61fe4197c72d361d1f312a01feb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/5e/fb/38ca2bae62f9d07f22d246c55dd1f4721ac40f82dc6c2f348c\n",
            "Successfully built gpyopt\n",
            "Installing collected packages: scipy, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, Mako, lightning-utilities, colorlog, paramz, nvidia-cusparse-cu12, nvidia-cudnn-cu12, alembic, optuna, nvidia-cusolver-cu12, GPy, ray, gpyopt, torchmetrics, torch-ema, pytorch-lightning\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed GPy-1.13.2 Mako-1.3.8 alembic-1.14.1 colorlog-6.9.0 gpyopt-1.2.6 lightning-utilities-0.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 optuna-4.2.0 paramz-0.9.6 pytorch-lightning-2.5.0.post0 ray-2.41.0 scipy-1.12.0 torch-ema-0.3 torchmetrics-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning ray optuna torch transformers scikit-learn tqdm gpyopt seaborn matplotlib pandas torchmetrics torch-ema pyyaml ray"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sampling.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6NCsjiy9ml3",
        "outputId": "a0950d42-852a-4074-8445-faa775ee3274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original size: 60875\n",
            "Sampled size: 15215\n",
            "\n",
            "Rating distribution in sampled dataset:\n",
            "rating\n",
            "1    3043\n",
            "2    3043\n",
            "3    3043\n",
            "4    3043\n",
            "5    3043\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = \"results\"\n",
        "\n",
        "# Delete the folder and its contents\n",
        "!rm -rf {folder_name}\n",
        "\n",
        "print(f\"Folder '{folder_name}' deleted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SozHdGo__0G",
        "outputId": "65621172-8764-4ddc-ba90-c9614e129604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder 'results' deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python training_script.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1Tc11QBXMQb",
        "outputId": "f41d9cd3-3b30-42b5-ba8f-58e362b95911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing ray_trainer.py\n",
            "ray_trainer.py loaded successfully\n",
            "INFO:__main__:Starting main function\n",
            "INFO:utils:Starting load_config from path: config.yaml\n",
            "INFO:utils:Successfully loaded config from: /content/config.yaml\n",
            "INFO:utils:Exiting load_config\n",
            "INFO:__main__:Configuration loaded successfully.\n",
            "INFO:utils:Starting create_directories\n",
            "INFO:utils:Directory already exists: results\n",
            "INFO:utils:Directory already exists: results/cached_models\n",
            "INFO:utils:Directory already exists: results/trained_models\n",
            "INFO:utils:Directory already exists: results/processed_data\n",
            "INFO:utils:Directory already exists: results/plots\n",
            "INFO:utils:Successfully created/verified directories.\n",
            "INFO:utils:Exiting create_directories\n",
            "INFO:__main__:Directories created/verified.\n",
            "Initializing RayTrainer\n",
            "Starting Ray...\n",
            "2025-01-31 22:14:03,174\tDEBUG node.py:293 -- Setting node ID to e83b11281218735eaf17428a39d6bed66f907af0794cbfee2861b1e2\n",
            "2025-01-31 22:14:03,176\tDEBUG node.py:1409 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2025-01-31_22-14-03_172653_83887/logs.\n",
            "2025-01-31 22:14:04,441\tDEBUG node.py:1438 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2025-01-31_22-14-03_172653_83887/logs.\n",
            "2025-01-31 22:14:04,442\tDEBUG npu.py:60 -- Could not import AscendCL: No module named 'acl'\n",
            "2025-01-31 22:14:04,479\tDEBUG tpu.py:115 -- Failed to detect number of TPUs: [Errno 2] No such file or directory: '/dev/vfio'\n",
            "2025-01-31 22:14:04,481\tDEBUG services.py:2131 -- Determine to start the Plasma object store with 16.95 GB memory using /dev/shm.\n",
            "2025-01-31 22:14:04,552\tINFO worker.py:1841 -- Started a local Ray instance.\n",
            "Ray initialization info: {'accelerator_type:L4': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'memory': 33901404980.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 16950702489.0, 'GPU': 1.0}\n",
            "\n",
            "=== Starting Training Pipeline ===\n",
            "Available resources: {'accelerator_type:L4': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'memory': 33901404980.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 16950702489.0, 'GPU': 1.0}\n",
            "Total models to train: 4\n",
            "Creating workers for 4 models\n",
            "Creating worker for distilbert-base-uncased\n",
            ":job_id:01000000\n",
            ":job_id:01000000\n",
            ":actor_name:RayTrainerWorker\n",
            ":actor_name:RayTrainerWorker\n",
            "Worker initialized with PID: 83887\n",
            "Worker created for distilbert-base-uncased\n",
            "Creating worker for google/electra-base-discriminator\n",
            ":actor_name:RayTrainerWorker\n",
            ":actor_name:RayTrainerWorker\n",
            "Worker initialized with PID: 83887\n",
            "Worker created for google/electra-base-discriminator\n",
            "Creating worker for bert-base-uncased\n",
            ":actor_name:RayTrainerWorker\n",
            ":actor_name:RayTrainerWorker\n",
            "Worker initialized with PID: 83887\n",
            "Worker created for bert-base-uncased\n",
            "Creating worker for roberta-base\n",
            ":actor_name:RayTrainerWorker\n",
            ":actor_name:RayTrainerWorker\n",
            "Worker initialized with PID: 83887\n",
            "Worker created for roberta-base\n",
            "Launching training tasks\n",
            "Launching task for distilbert-base-uncased\n",
            "\n",
            "Starting training for distilbert-base-uncased\n",
            "INFO:data_module:Starting AdvancedDataModule initialization for model: distilbert-base-uncased\n",
            "INFO:data_module:AdvancedDataModule initialized with model: distilbert-base-uncased, batch_size: 60\n",
            "INFO:data_module:Exiting AdvancedDataModule initialization for model: distilbert-base-uncased\n",
            "distilbert-base-uncased: Data module initialized\n",
            "INFO:data_module:Starting prepare_data for model: distilbert-base-uncased\n",
            "INFO:data_module:Tokenizer not found locally, downloading distilbert-base-uncased tokenizer to results/cached_models/distilbert-base-uncased...\n",
            "INFO:data_module:Tokenizer for distilbert-base-uncased loaded successfully.\n",
            "INFO:data_module:Pre-processed data not found at results/processed_data/distilbert-base-uncased/processed_data.pt, starting data processing.\n",
            "INFO:data_module:Starting _process_data for model: distilbert-base-uncased\n",
            "INFO:data_module:CSV file read successfully, shape: (15215, 2)\n",
            "INFO:data_module:Splitting data into train, val, test sets (stratified).\n",
            "INFO:data_module:Data split complete. Train: 12172, Val: 1521, Test: 1522 samples.\n",
            "INFO:data_module:Tokenizing data using distilbert-base-uncased tokenizer...\n",
            "INFO:data_module:Tokenization complete.\n",
            "INFO:data_module:Created directory: results/processed_data/distilbert-base-uncased\n",
            "INFO:data_module:Processed data saved successfully to: results/processed_data/distilbert-base-uncased/processed_data.pt\n",
            "INFO:data_module:Exiting _process_data for model: distilbert-base-uncased\n",
            "processed_filepath SAVE: results/processed_data/distilbert-base-uncased/processed_data.pt\n",
            "INFO:data_module:Exiting prepare_data for model: distilbert-base-uncased\n",
            "distilbert-base-uncased: Data prepared\n",
            "INFO:data_module:Starting setup for stage: None, model: distilbert-base-uncased\n",
            "INFO:data_module:Loading processed data from: results/processed_data/distilbert-base-uncased/processed_data.pt\n",
            "processed_filepath LOAD: results/processed_data/distilbert-base-uncased/processed_data.pt\n",
            "INFO:dataset:Starting AdvancedDataset initialization\n",
            "INFO:dataset:AdvancedDataset initialized with 12172 samples.\n",
            "INFO:dataset:Exiting AdvancedDataset initialization\n",
            "INFO:dataset:Starting AdvancedDataset initialization\n",
            "INFO:dataset:AdvancedDataset initialized with 1521 samples.\n",
            "INFO:dataset:Exiting AdvancedDataset initialization\n",
            "INFO:dataset:Starting AdvancedDataset initialization\n",
            "INFO:dataset:AdvancedDataset initialized with 1522 samples.\n",
            "INFO:dataset:Exiting AdvancedDataset initialization\n",
            "INFO:data_module:Datasets created: train=12172, val=1521, test=1522.\n",
            "INFO:data_module:Exiting setup for model: distilbert-base-uncased, stage: None\n",
            "distilbert-base-uncased: Setup completed\n",
            "\n",
            "Initializing model: distilbert-base-uncased\n",
            "Learning rate: 2e-05\n",
            "Cached model directory: results/cached_models\n",
            "INFO:simple_model:Starting SimpleTransformerClassifier initialization for model: distilbert-base-uncased\n",
            "INFO:simple_model:Loading configuration for distilbert-base-uncased\n",
            "INFO:simple_model:Loading model distilbert-base-uncased from Hugging Face hub\n",
            "2025-01-31 22:14:14.131245: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-01-31 22:14:14.149551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738361654.171536   83887 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738361654.178029   83887 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-31 22:14:14.198457: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:simple_model:Model initialized successfully with 5 classes\n",
            "INFO:simple_model:Exiting SimpleTransformerClassifier initialization for model: distilbert-base-uncased\n",
            "distilbert-base-uncased: Model initialized\n",
            "/usr/local/lib/python3.11/dist-packages/lightning_fabric/connector.py:572: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "distilbert-base-uncased: Trainer configured\n",
            "INFO:data_module:Starting prepare_data for model: distilbert-base-uncased\n",
            "INFO:data_module:Tokenizer not found locally, downloading distilbert-base-uncased tokenizer to results/cached_models/distilbert-base-uncased...\n",
            "INFO:data_module:Tokenizer for distilbert-base-uncased loaded successfully.\n",
            "INFO:data_module:Pre-processed data not found at results/processed_data/distilbert-base-uncased/processed_data.pt, starting data processing.\n",
            "INFO:data_module:Starting _process_data for model: distilbert-base-uncased\n",
            "INFO:data_module:CSV file read successfully, shape: (15215, 2)\n",
            "INFO:data_module:Splitting data into train, val, test sets (stratified).\n",
            "INFO:data_module:Data split complete. Train: 12172, Val: 1521, Test: 1522 samples.\n",
            "INFO:data_module:Tokenizing data using distilbert-base-uncased tokenizer...\n",
            "INFO:data_module:Tokenization complete.\n",
            "INFO:data_module:Created directory: results/processed_data/distilbert-base-uncased\n",
            "INFO:data_module:Processed data saved successfully to: results/processed_data/distilbert-base-uncased/processed_data.pt\n",
            "INFO:data_module:Exiting _process_data for model: distilbert-base-uncased\n",
            "processed_filepath SAVE: results/processed_data/distilbert-base-uncased/processed_data.pt\n",
            "INFO:data_module:Exiting prepare_data for model: distilbert-base-uncased\n",
            "INFO:data_module:Starting setup for stage: TrainerFn.FITTING, model: distilbert-base-uncased\n",
            "INFO:data_module:Loading processed data from: results/processed_data/distilbert-base-uncased/processed_data.pt\n",
            "processed_filepath LOAD: results/processed_data/distilbert-base-uncased/processed_data.pt\n",
            "INFO:dataset:Starting AdvancedDataset initialization\n",
            "INFO:dataset:AdvancedDataset initialized with 12172 samples.\n",
            "INFO:dataset:Exiting AdvancedDataset initialization\n",
            "INFO:dataset:Starting AdvancedDataset initialization\n",
            "INFO:dataset:AdvancedDataset initialized with 1521 samples.\n",
            "INFO:dataset:Exiting AdvancedDataset initialization\n",
            "INFO:dataset:Starting AdvancedDataset initialization\n",
            "INFO:dataset:AdvancedDataset initialized with 1522 samples.\n",
            "INFO:dataset:Exiting AdvancedDataset initialization\n",
            "INFO:data_module:Datasets created: train=12172, val=1521, test=1522.\n",
            "INFO:data_module:Exiting setup for model: distilbert-base-uncased, stage: TrainerFn.FITTING\n",
            "INFO:simple_model:Configuring optimizers for model: distilbert-base-uncased with learning rate: 2e-05\n",
            "INFO:simple_model:AdamW optimizer configured.\n",
            "INFO:simple_model:Exiting configure_optimizers for model: distilbert-base-uncased\n",
            "INFO:data_module:val_dataloader requested for model: distilbert-base-uncased\n",
            "INFO:data_module:Validation dataloader created with batch size: 60, num_workers=4.\n",
            "INFO:data_module:Exiting val_dataloader for model: distilbert-base-uncased\n",
            "INFO:simple_model:Validation started - model set to eval mode\n",
            "\n",
            "=== Validation Started ===\n",
            "Validation batch 0\n",
            "Forward pass shapes:\n",
            " - input_ids: torch.Size([60, 128])\n",
            " - attention_mask: torch.Size([60, 128])\n",
            " - labels: torch.Size([60])\n",
            "Forward pass outputs:\n",
            " - loss: 1.6236\n",
            " - logits shape: torch.Size([60, 5])\n",
            "\n",
            "Val Batch 0 - Loss: 1.6236, Acc: 0.1500\n",
            "\n",
            "Forward pass shapes:\n",
            " - input_ids: torch.Size([60, 128])\n",
            " - attention_mask: torch.Size([60, 128])\n",
            " - labels: torch.Size([60])\n",
            "Forward pass outputs:\n",
            " - loss: 1.6229\n",
            " - logits shape: torch.Size([60, 5])\n",
            "INFO:simple_model:Starting on_validation_epoch_end for epoch 0, model: distilbert-base-uncased\n",
            "\n",
            "Epoch 0 Summary:\n",
            " - Train Loss: nan\n",
            " - Train Acc: nan\n",
            " - Val Loss: nan\n",
            " - Val Acc: nan\n",
            "INFO:simple_model:Exiting on_validation_epoch_end for epoch 0, model: distilbert-base-uncased\n",
            "INFO:data_module:train_dataloader requested for model: distilbert-base-uncased\n",
            "INFO:data_module:Train dataloader created with batch size: 60, shuffle=True, num_workers=4.\n",
            "INFO:data_module:Exiting train_dataloader for model: distilbert-base-uncased\n",
            "INFO:simple_model:Training started - model set to train mode\n",
            "\n",
            "=== Training Started ===\n",
            "Training batch 0\n",
            "Batch shapes - input_ids: torch.Size([60, 128]), attention_mask: torch.Size([60, 128]), labels: torch.Size([60])\n",
            "\n",
            "Forward pass shapes:\n",
            " - input_ids: torch.Size([60, 128])\n",
            " - attention_mask: torch.Size([60, 128])\n",
            " - labels: torch.Size([60])\n",
            "Forward pass outputs:\n",
            " - loss: 1.6071\n",
            " - logits shape: torch.Size([60, 5])\n",
            "\n",
            "Batch 0 - Loss: 1.6071, Acc: 0.2333, Grad Norm: 0.0000\n",
            "\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
            "Task launched for distilbert-base-uncased\n",
            "Launching task for google/electra-base-discriminator\n",
            "Task launched for google/electra-base-discriminator\n",
            "Launching task for bert-base-uncased\n",
            "Task launched for bert-base-uncased\n",
            "Launching task for roberta-base\n",
            "Task launched for roberta-base\n",
            "Waiting for 4 tasks to complete\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def download_py_files():\n",
        "    \"\"\"Downloads all .py files from the Colab environment to your local machine.\"\"\"\n",
        "\n",
        "    py_files = [f for f in os.listdir('.') if f.endswith('.py')]\n",
        "\n",
        "    if not py_files:\n",
        "        print(\"No .py files found in the current directory.\")\n",
        "        return\n",
        "\n",
        "    for filename in py_files:\n",
        "        try:\n",
        "            files.download(filename)\n",
        "            print(f\"Downloaded: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {filename}: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Example usage when run in Colab:\n",
        "  download_py_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "5wzmvXp7XMSi",
        "outputId": "e279f14f-af4f-4569-f3d8-787a71e0756e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b2b02259-13c4-474d-8c17-2361dfbe62ca\", \"progress_bar.py\", 1223)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: progress_bar.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_88266732-cb7e-4934-b927-26f3ef4b0ee0\", \"data_module.py\", 11034)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: data_module.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d697422a-1a2e-4998-ac73-7405c0606c71\", \"training_script.py\", 3674)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: training_script.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1dca60ef-970b-406a-8a79-8df60fcb7430\", \"dataset.py\", 1420)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: dataset.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_949b116c-5764-45f9-984a-e2f67f61b723\", \"simple_model.py\", 10092)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: simple_model.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_03d53f46-6a94-4f08-b9d2-a6251c9cb2d5\", \"ray_trainer.py\", 6231)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: ray_trainer.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0eacd411-f2d5-4cfb-a940-5192f3a6822a\", \"sampling.py\", 1478)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: sampling.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9446c8fa-adfb-4a7d-97c6-8721f6dba427\", \"utils.py\", 4100)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: utils.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_658288f0-9c3a-4b96-aa51-8e34b360cf88\", \"embeddings.py\", 3249)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efS6W6KCXMXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rlE0H8jsXMZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPyEti0FXMa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PumuKKHUXMdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Load your data (replace with your actual loading method)\n",
        "    import pandas as pd\n",
        "    from embeddings import TextProcessorConfig, GPUEmbeddingGenerator\n",
        "    df = pd.read_csv('so_many_rev.csv') # Example path\n",
        "\n",
        "    # Initialize GPUEmbeddingGenerator\n",
        "    config_gpu = TextProcessorConfig() # Use default config, you can customize if needed\n",
        "    gpu_processor = GPUEmbeddingGenerator(config=config_gpu)\n",
        "\n",
        "    # Generate and save embeddings\n",
        "    gpu_processor.generate_and_save_embeddings_by_rating(df, text_column='text', rating_column='rating')\n",
        "\n",
        "    print(\"\\nGPU Embedding Generation Complete. Embeddings saved in:\", config_gpu.output_embeddings_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiF_Xr0FXMer",
        "outputId": "a58469ef-3813-44c0-8394-3e4ae970f259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for rating 1...\n",
            "Embeddings for rating 1 saved to: output_embeddings/rating_1_embeddings.npy\n",
            "Generating embeddings for rating 2...\n",
            "Embeddings for rating 2 saved to: output_embeddings/rating_2_embeddings.npy\n",
            "Generating embeddings for rating 3...\n",
            "Embeddings for rating 3 saved to: output_embeddings/rating_3_embeddings.npy\n",
            "Generating embeddings for rating 4...\n",
            "Embeddings for rating 4 saved to: output_embeddings/rating_4_embeddings.npy\n",
            "Generating embeddings for rating 5...\n",
            "Embeddings for rating 5 saved to: output_embeddings/rating_5_embeddings.npy\n",
            "\n",
            "GPU Embedding Generation Complete. Embeddings saved in: output_embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'folder_name' with the name of your folder\n",
        "folder_name = 'output_embeddings'\n",
        "\n",
        "# Create a zip file of the folder\n",
        "shutil.make_archive(folder_name, 'zip', folder_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wxI8UsNAXMiV",
        "outputId": "4a0bfa3d-e60c-4fa8-830a-08bf9678232f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/output_embeddings.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ElectraTokenizer, ElectraModel, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "from huggingface_hub import model_info\n",
        "from huggingface_hub.utils import HfHubHTTPError\n",
        "import os\n",
        "\n",
        "# Initialize models\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "st_model = SentenceTransformer('all-mpnet-base-v2') # Updated to all-mpnet-base-v2\n",
        "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
        "\n",
        "# Define ABSA model name and load using Auto classes\n",
        "absa_model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
        "model_loaded = False\n",
        "\n",
        "def check_model_name(model_name):\n",
        "    \"\"\"\n",
        "    Checks if a model name is valid and exists on Hugging Face Hub.\n",
        "    Returns True if the model exists, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model_info(model_name)\n",
        "        return True\n",
        "    except HfHubHTTPError as e:\n",
        "        if e.response.status_code == 404:\n",
        "            print(f\"Error: Model '{model_name}' not found on Hugging Face Hub.\")\n",
        "        else:\n",
        "            print(f\"Error checking model '{model_name}': {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "if check_model_name(absa_model_name):\n",
        "    try:\n",
        "        absa_tokenizer = AutoTokenizer.from_pretrained(absa_model_name)\n",
        "        absa_model = AutoModelForSequenceClassification.from_pretrained(absa_model_name).to('cuda')\n",
        "        model_loaded = True\n",
        "        print(\"ABSA Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ABSA model: {e}\")\n",
        "        print(f\"Failed to load ABSA model '{absa_model_name}'.\")\n",
        "else:\n",
        "    print(f\"Model name check failed for '{absa_model_name}'. ABSA functionalities will be skipped.\")\n",
        "\n",
        "class EnhancedElectra(nn.Module):\n",
        "    def __init__(self, feature_dim, num_classes=5, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        # ELECTRA output dimension is 768 for base model\n",
        "        electra_dim = 768 # Changed to 768 for base model\n",
        "\n",
        "        # Combined input dimension is ELECTRA output + additional features\n",
        "        combined_dim = electra_dim + feature_dim\n",
        "\n",
        "        self.layer1 = nn.Linear(combined_dim, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.layer2 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.layer3 = nn.Linear(512, 256)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "        # Initialize ELECTRA\n",
        "        self.electra = ElectraModel.from_pretrained('google/electra-base-discriminator') # Changed to base discriminator\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, features):\n",
        "        # Get ELECTRA outputs\n",
        "        electra_output = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = electra_output.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
        "\n",
        "        # Combine ELECTRA output with additional features\n",
        "        combined_features = torch.cat([pooled_output, features], dim=1)\n",
        "\n",
        "        # Forward through MLP layers\n",
        "        h1 = F.gelu(self.bn1(self.layer1(combined_features)))\n",
        "        h1 = self.dropout(h1)\n",
        "\n",
        "        h2 = F.gelu(self.bn2(self.layer2(h1)))\n",
        "        h2 = self.dropout(h2)\n",
        "\n",
        "        h3 = F.gelu(self.bn3(self.layer3(h2)))\n",
        "        h3 = self.dropout(h3)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.classifier(h3)\n",
        "        return output\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        return False\n",
        "\n",
        "class GPUOptimizedTrainer:\n",
        "    def __init__(self, texts, labels, hyperparams, val_split=0.2, feature_cache_dir=\"features_cache\"):\n",
        "        self.device = torch.device('cuda')\n",
        "        self.hyperparams = hyperparams\n",
        "        self.batch_size = hyperparams[\"Batch Size\"]\n",
        "        self.feature_cache_dir = feature_cache_dir\n",
        "        os.makedirs(self.feature_cache_dir, exist_ok=True)\n",
        "\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "        self.feature_dim = self.get_feature_dimension(texts[:1])\n",
        "        hyperparams[\"Feature Dimension\"] = self.feature_dim\n",
        "        print(f\"Feature Dimension: {hyperparams['Feature Dimension']}\")\n",
        "\n",
        "        self.model = EnhancedElectra(num_classes=5, feature_dim=hyperparams[\"Feature Dimension\"],\n",
        "                                   dropout_rate=hyperparams[\"Dropout\"]).to(self.device)\n",
        "\n",
        "        # Updated to use new GradScaler syntax\n",
        "        self.scaler = torch.amp.GradScaler('cuda')\n",
        "        self.prepare_data(texts, labels, val_split)\n",
        "        self.setup_training()\n",
        "\n",
        "    def get_feature_dimension(self, sample_texts):\n",
        "        return self.extract_features(sample_texts).shape[1]\n",
        "\n",
        "    def extract_features(self, texts):\n",
        "        print(\"Extracting features...\")\n",
        "        def extract_contextual(texts, batch_size=128):\n",
        "            features = []\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch = texts[i:i + batch_size]\n",
        "                with torch.no_grad():\n",
        "                    embeddings = st_model.encode(batch, convert_to_tensor=True)\n",
        "                    features.extend(embeddings.cpu().numpy())\n",
        "            return np.array(features)\n",
        "\n",
        "        def extract_syntactic(texts):\n",
        "            features = []\n",
        "            for text in texts:\n",
        "                doc = nlp(text)\n",
        "                pos_tags = [token.pos_ for token in doc]\n",
        "                features.append([len(doc), len(set(pos_tags)) / len(pos_tags),\n",
        "                               pos_tags.count('NOUN') / len(pos_tags),\n",
        "                               pos_tags.count('VERB') / len(pos_tags)])\n",
        "            return np.array(features)\n",
        "\n",
        "        contextual_features = extract_contextual(texts)\n",
        "        syntactic_features = extract_syntactic(texts)\n",
        "        return np.hstack([contextual_features, syntactic_features])\n",
        "\n",
        "    def prepare_data(self, texts, labels, val_split):\n",
        "        print(\"Preparing data...\")\n",
        "        encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
        "        input_ids = encodings['input_ids']\n",
        "        attention_mask = encodings['attention_mask']\n",
        "\n",
        "        split_idx = int(len(input_ids) * (1 - val_split))\n",
        "        indices = np.random.permutation(len(input_ids))\n",
        "        train_idx = indices[:split_idx]\n",
        "        val_idx = indices[split_idx:]\n",
        "\n",
        "        self.train_input_ids = input_ids[train_idx]\n",
        "        self.train_attention_mask = attention_mask[train_idx]\n",
        "        self.val_input_ids = input_ids[val_idx]\n",
        "        self.val_attention_mask = attention_mask[val_idx]\n",
        "\n",
        "        train_feature_path = os.path.join(self.feature_cache_dir, \"train_features.pt\")\n",
        "        val_feature_path = os.path.join(self.feature_cache_dir, \"val_features.pt\")\n",
        "\n",
        "        if os.path.exists(train_feature_path) and os.path.exists(val_feature_path):\n",
        "            print(\"Loading features from cache...\")\n",
        "            # Updated to use weights_only=True for security\n",
        "            self.train_features = torch.load(train_feature_path, weights_only=True)\n",
        "            self.val_features = torch.load(val_feature_path, weights_only=True)\n",
        "            print(\"Features loaded from cache.\")\n",
        "        else:\n",
        "            print(\"Extracting and caching features...\")\n",
        "            features = self.extract_features(texts)\n",
        "            self.train_features = torch.tensor(features[train_idx], dtype=torch.float32)\n",
        "            self.val_features = torch.tensor(features[val_idx], dtype=torch.float32)\n",
        "            torch.save(self.train_features, train_feature_path)\n",
        "            torch.save(self.val_features, val_feature_path)\n",
        "            print(f\"Features cached to {train_feature_path} and {val_feature_path}\")\n",
        "\n",
        "        self.train_labels = torch.tensor(labels[train_idx], dtype=torch.long)\n",
        "        self.val_labels = torch.tensor(labels[val_idx], dtype=torch.long)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        self.train_features = torch.tensor(scaler.fit_transform(self.train_features), dtype=torch.float32)\n",
        "        self.val_features = torch.tensor(scaler.transform(self.val_features), dtype=torch.float32)\n",
        "\n",
        "        self.create_dataloaders()\n",
        "\n",
        "    def create_dataloaders(self):\n",
        "        train_dataset = TensorDataset(self.train_input_ids, self.train_attention_mask,\n",
        "                                    self.train_features, self.train_labels)\n",
        "        val_dataset = TensorDataset(self.val_input_ids, self.val_attention_mask,\n",
        "                                  self.val_features, self.val_labels)\n",
        "\n",
        "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size,\n",
        "                                     shuffle=True, pin_memory=True,\n",
        "                                     num_workers=4, prefetch_factor=3,\n",
        "                                     persistent_workers=True)\n",
        "        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size * 2,\n",
        "                                   pin_memory=True, num_workers=4)\n",
        "\n",
        "    def setup_training(self):\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=self.hyperparams[\"Learning Rate\"],\n",
        "            weight_decay=self.hyperparams[\"Weight Decay\"],\n",
        "            betas=(0.9, 0.999)\n",
        "        )\n",
        "\n",
        "        # Calculate total steps for the scheduler\n",
        "        num_epochs = self.hyperparams[\"Epochs\"]\n",
        "        num_training_steps = len(self.train_loader) * num_epochs\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            self.optimizer,\n",
        "            max_lr=2 * self.hyperparams[\"Learning Rate\"],\n",
        "            total_steps=num_training_steps,\n",
        "            epochs=None,\n",
        "            steps_per_epoch=None,\n",
        "            pct_start=0.1\n",
        "        )\n",
        "\n",
        "        # Initialize early stopping\n",
        "        self.early_stopping = EarlyStopping(\n",
        "            patience=self.hyperparams[\"Early Stopping Patience\"],\n",
        "            min_delta=0\n",
        "        )\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (input_ids, attention_mask, features, target) in enumerate(self.train_loader):\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "            features = features.to(self.device, non_blocking=True)\n",
        "            target = target.to(self.device, non_blocking=True)\n",
        "\n",
        "            # Updated to use new autocast syntax\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                output = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                loss = F.cross_entropy(output, target)\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "            self.optimizer.zero_grad(set_to_none=True)\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "        return {'loss': total_loss / len(self.train_loader), 'acc': correct / total}\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, features, target in self.val_loader:\n",
        "                input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "                attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "                features = features.to(self.device, non_blocking=True)\n",
        "                target = target.to(self.device, non_blocking=True)\n",
        "\n",
        "                # Updated to use new autocast syntax\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    output = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                    loss = F.cross_entropy(output, target)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "        return {'loss': total_loss / len(self.val_loader), 'acc': correct / total}\n",
        "\n",
        "    def train(self, epochs=None):\n",
        "        if epochs is None:\n",
        "            epochs = self.hyperparams[\"Epochs\"]\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        train_losses, val_losses = [], []\n",
        "        train_accs, val_accs = [], []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_metrics = self.train_epoch()\n",
        "            val_metrics = self.validate()\n",
        "\n",
        "            train_losses.append(train_metrics['loss'])\n",
        "            val_losses.append(val_metrics['loss'])\n",
        "            train_accs.append(train_metrics['acc'])\n",
        "            val_accs.append(val_metrics['acc'])\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            print(f\"Train Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['acc']:.4f}\")\n",
        "            print(f\"Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
        "\n",
        "            if val_metrics['loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['loss']\n",
        "                torch.save(self.model.state_dict(), 'best_model.pt')\n",
        "\n",
        "            if self.early_stopping(val_metrics['loss']):\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'train_loss': train_losses,\n",
        "            'val_loss': val_losses,\n",
        "            'train_acc': train_accs,\n",
        "            'val_acc': val_accs\n",
        "        }\n",
        "\n",
        "def extract_keywords(text, nlp_model=nlp, top_n=5):\n",
        "    doc = nlp_model(text)\n",
        "    keywords = [token.text for token in doc if token.pos_ in ['NOUN', 'ADJ', 'VERB'] and not token.is_stop]\n",
        "    return keywords[:top_n]\n",
        "\n",
        "def extract_ner(text, nlp_model=nlp):\n",
        "    doc = nlp_model(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "def perform_absa_zero_shot(text, aspects, absa_model=absa_model, absa_tokenizer=absa_tokenizer, device='cuda'):\n",
        "    if not model_loaded:\n",
        "        print(\"ABSA model was not loaded. Skipping ABSA.\")\n",
        "        return {}\n",
        "\n",
        "    aspect_sentiments = {}\n",
        "    for aspect in aspects:\n",
        "        inputs = absa_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = absa_model(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=-1)\n",
        "            sentiment_id = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "        sentiment_label = absa_model.config.id2label[sentiment_id] if hasattr(absa_model.config, 'id2label') else str(sentiment_id)\n",
        "        aspect_sentiments[aspect] = sentiment_label\n",
        "    return aspect_sentiments\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_path = \"/content/processed_df.csv\"\n",
        "    df = pd.read_csv(data_path)\n",
        "    df['text'] = df['modified_text']\n",
        "\n",
        "    df[\"label\"] = df[\"rating\"] - 1\n",
        "    texts = df[\"text\"].tolist()\n",
        "    labels = np.array(df[\"label\"].tolist())\n",
        "\n",
        "    hyperparams = {\n",
        "        \"Epochs\": 40,\n",
        "        \"Batch Size\": 32,\n",
        "        \"Learning Rate\": 1e-5,\n",
        "        \"Dropout\": 0.3,\n",
        "        \"Weight Decay\": 0.1,\n",
        "        \"Label Smoothing\": 0.1,\n",
        "        \"Early Stopping Patience\": 5,\n",
        "        \"Gradient Accumulation Steps\": 4,\n",
        "        \"Optimizer\": \"AdamW\",\n",
        "        \"Scheduler\": \"OneCycleLR\",\n",
        "        \"Feature Dimension\": 768 + 4, # Updated Feature Dimension calculation - important!\n",
        "        \"Model\": \"Electra-base\" # Updated Model Name in Hyperparams\n",
        "    }\n",
        "\n",
        "    sample_text = texts[0]\n",
        "    print(f\"Sample Text: {sample_text}\\n\")\n",
        "\n",
        "    keywords = extract_keywords(sample_text)\n",
        "    print(f\"Keywords: {keywords}\\n\")\n",
        "\n",
        "    entities = extract_ner(sample_text)\n",
        "    print(f\"Named Entities: {entities}\\n\")\n",
        "\n",
        "    aspects_for_absa = keywords\n",
        "    if model_loaded:\n",
        "        aspect_sentiments = perform_absa_zero_shot(sample_text, aspects_for_absa)\n",
        "        print(f\"Aspect-Based Sentiment Analysis: {aspect_sentiments}\\n\")\n",
        "    else:\n",
        "        print(\"Skipping ABSA due to model loading failure.\\n\")\n",
        "\n",
        "    trainer = GPUOptimizedTrainer(texts, labels, hyperparams=hyperparams)\n",
        "    metrics = trainer.train()\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(metrics['train_loss'], label='Train Loss')\n",
        "    plt.plot(metrics['val_loss'], label='Val Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(metrics['train_acc'], label='Train Acc')\n",
        "    plt.plot(metrics['val_acc'], label='Val Acc')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oXHGSuvZPmYS",
        "outputId": "de808ee5-43d3-44be-9bc0-942e94479d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ABSA Model loaded successfully!\n",
            "Sample Text: terrible sent gate arm defective fortunately service man able exchange one worked\n",
            "\n",
            "Keywords: ['terrible', 'sent', 'gate', 'arm', 'defective']\n",
            "\n",
            "Named Entities: []\n",
            "\n",
            "Aspect-Based Sentiment Analysis: {'terrible': 'Negative', 'sent': 'Negative', 'gate': 'Negative', 'arm': 'Negative', 'defective': 'Negative'}\n",
            "\n",
            "Extracting features...\n",
            "Feature Dimension: 772\n",
            "Preparing data...\n",
            "Loading features from cache...\n",
            "Features loaded from cache.\n",
            "Epoch 1/40\n",
            "Train Loss: 1.6764, Acc: 0.2058\n",
            "Val Loss: 1.6234, Acc: 0.2054\n",
            "Epoch 2/40\n",
            "Train Loss: 1.6360, Acc: 0.2348\n",
            "Val Loss: 1.4677, Acc: 0.3526\n",
            "Epoch 3/40\n",
            "Train Loss: 1.3308, Acc: 0.4153\n",
            "Val Loss: 1.1759, Acc: 0.4935\n",
            "Epoch 4/40\n",
            "Train Loss: 1.1168, Acc: 0.5233\n",
            "Val Loss: 1.0930, Acc: 0.5304\n",
            "Epoch 5/40\n",
            "Train Loss: 1.0214, Acc: 0.5708\n",
            "Val Loss: 1.1000, Acc: 0.5411\n",
            "Epoch 6/40\n",
            "Train Loss: 0.9317, Acc: 0.6167\n",
            "Val Loss: 1.0910, Acc: 0.5463\n",
            "Epoch 7/40\n",
            "Train Loss: 0.8532, Acc: 0.6577\n",
            "Val Loss: 1.1333, Acc: 0.5411\n",
            "Epoch 8/40\n",
            "Train Loss: 0.7994, Acc: 0.6802\n",
            "Val Loss: 1.1873, Acc: 0.5361\n",
            "Epoch 9/40\n",
            "Train Loss: 0.7788, Acc: 0.6876\n",
            "Val Loss: 1.2151, Acc: 0.5432\n",
            "Epoch 10/40\n",
            "Train Loss: 0.6883, Acc: 0.7351\n",
            "Val Loss: 1.3297, Acc: 0.5396\n",
            "Epoch 11/40\n",
            "Train Loss: 0.6073, Acc: 0.7715\n",
            "Val Loss: 1.3855, Acc: 0.5290\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel  # Change to BERT\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Initialize models\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'lemmatizer'])\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Change to BERT tokenizer\n",
        "\n",
        "class RegularizedBERT(nn.Module):\n",
        "    def __init__(self, num_labels, feature_dim):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')  # Change to BERT\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768 + feature_dim, 384),  # BERT output size is 768\n",
        "            nn.LayerNorm(384),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(384, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, features):\n",
        "        # Get BERT output\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n",
        "\n",
        "        # Concatenate with engineered features\n",
        "        combined_features = torch.cat([pooled_output, features], dim=1)\n",
        "\n",
        "        # Pass through classifier\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n",
        "\n",
        "class GPUOptimizedTrainer:\n",
        "    def __init__(self, texts, labels, batch_size=32, val_split=0.2):\n",
        "        self.device = torch.device('cuda')\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "        # Extract features to determine feature dimension\n",
        "        self.feature_dim = self.extract_features(texts[:1]).shape[1]  # Get feature dimension from a single sample\n",
        "\n",
        "        self.model = RegularizedBERT(num_labels=5, feature_dim=self.feature_dim).to(self.device)\n",
        "\n",
        "        self.scaler = GradScaler()\n",
        "        self.prepare_data(texts, labels, val_split)\n",
        "        self.setup_training()\n",
        "\n",
        "    def extract_features(self, texts):\n",
        "        print(\"Extracting features...\")\n",
        "\n",
        "        def extract_contextual(texts, batch_size=128):\n",
        "            features = []\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch = texts[i:i + batch_size]\n",
        "                with torch.no_grad():\n",
        "                    embeddings = st_model.encode(batch, convert_to_tensor=True)\n",
        "                    features.extend(embeddings.cpu().numpy())\n",
        "            return np.array(features)\n",
        "\n",
        "        def extract_syntactic(texts):\n",
        "            features = []\n",
        "            for text in texts:\n",
        "                doc = nlp(text)\n",
        "                pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "                features.append([\n",
        "                    len(doc),  # document length\n",
        "                    len(set(pos_tags)) / len(pos_tags),  # POS diversity\n",
        "                    pos_tags.count('NOUN') / len(pos_tags),  # noun ratio\n",
        "                    pos_tags.count('VERB') / len(pos_tags),  # verb ratio\n",
        "                ])\n",
        "            return np.array(features)\n",
        "\n",
        "        contextual_features = extract_contextual(texts)\n",
        "        syntactic_features = extract_syntactic(texts)\n",
        "\n",
        "        return np.hstack([contextual_features, syntactic_features])\n",
        "\n",
        "    def prepare_data(self, texts, labels, val_split):\n",
        "        print(\"Preparing data...\")\n",
        "\n",
        "        # Tokenize the text data using BERT tokenizer\n",
        "        encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
        "        input_ids = encodings['input_ids']\n",
        "        attention_mask = encodings['attention_mask']\n",
        "\n",
        "        # Extract engineered features\n",
        "        features = self.extract_features(texts)\n",
        "\n",
        "        # Normalize features\n",
        "        scaler = StandardScaler()\n",
        "        features = scaler.fit_transform(features)\n",
        "\n",
        "        # Split data\n",
        "        split_idx = int(len(input_ids) * (1 - val_split))\n",
        "        indices = np.random.permutation(len(input_ids))\n",
        "\n",
        "        train_idx = indices[:split_idx]\n",
        "        val_idx = indices[split_idx:]\n",
        "\n",
        "        self.train_input_ids = input_ids[train_idx]\n",
        "        self.train_attention_mask = attention_mask[train_idx]\n",
        "        self.train_features = torch.tensor(features[train_idx], dtype=torch.float32)\n",
        "        self.train_labels = torch.tensor(labels[train_idx], dtype=torch.long)\n",
        "\n",
        "        self.val_input_ids = input_ids[val_idx]\n",
        "        self.val_attention_mask = attention_mask[val_idx]\n",
        "        self.val_features = torch.tensor(features[val_idx], dtype=torch.float32)\n",
        "        self.val_labels = torch.tensor(labels[val_idx], dtype=torch.long)\n",
        "\n",
        "        self.create_dataloaders()\n",
        "\n",
        "    def create_dataloaders(self):\n",
        "        train_dataset = TensorDataset(self.train_input_ids, self.train_attention_mask, self.train_features, self.train_labels)\n",
        "        val_dataset = TensorDataset(self.val_input_ids, self.val_attention_mask, self.val_features, self.val_labels)\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            pin_memory=True,\n",
        "            num_workers=4,\n",
        "            prefetch_factor=3,\n",
        "            persistent_workers=True\n",
        "        )\n",
        "\n",
        "        self.val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.batch_size * 2,\n",
        "            pin_memory=True,\n",
        "            num_workers=4\n",
        "        )\n",
        "\n",
        "    def setup_training(self):\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=1e-5,\n",
        "            weight_decay=0.1,\n",
        "            betas=(0.9, 0.999)\n",
        "        )\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            self.optimizer,\n",
        "            max_lr=2e-5,\n",
        "            epochs=50,\n",
        "            steps_per_epoch=len(self.train_loader),\n",
        "            pct_start=0.1\n",
        "        )\n",
        "\n",
        "        self.early_stopping = EarlyStopping(patience=3)\n",
        "\n",
        "    def train(self, epochs=15):\n",
        "        best_val_loss = float('inf')\n",
        "        train_losses, val_losses = [], []\n",
        "        train_accs, val_accs = [], []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_metrics = self.train_epoch()\n",
        "            val_metrics = self.validate()\n",
        "\n",
        "            train_losses.append(train_metrics['loss'])\n",
        "            val_losses.append(val_metrics['loss'])\n",
        "            train_accs.append(train_metrics['acc'])\n",
        "            val_accs.append(val_metrics['acc'])\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            print(f\"Train Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['acc']:.4f}\")\n",
        "            print(f\"Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
        "\n",
        "            if val_metrics['loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['loss']\n",
        "                torch.save(self.model.state_dict(), 'best_model.pt')\n",
        "\n",
        "            if self.early_stopping(val_metrics['loss']):\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "            self.scheduler.step()\n",
        "\n",
        "        return {\n",
        "            'train_loss': train_losses,\n",
        "            'val_loss': val_losses,\n",
        "            'train_acc': train_accs,\n",
        "            'val_acc': val_accs\n",
        "        }\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (input_ids, attention_mask, features, target) in enumerate(self.train_loader):\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "            features = features.to(self.device, non_blocking=True)\n",
        "            target = target.to(self.device, non_blocking=True)\n",
        "\n",
        "            with autocast():\n",
        "                output = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                loss = F.cross_entropy(output, target)\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "            self.optimizer.zero_grad(set_to_none=True)\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "        return {'loss': total_loss / len(self.train_loader), 'acc': correct / total}\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, features, target in self.val_loader:\n",
        "                input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "                attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "                features = features.to(self.device, non_blocking=True)\n",
        "                target = target.to(self.device, non_blocking=True)\n",
        "\n",
        "                with autocast():\n",
        "                    output = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                    loss = F.cross_entropy(output, target)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "        return {'loss': total_loss / len(self.val_loader), 'acc': correct / total}\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_path = \"/content/so_many_rev.csv\"\n",
        "    df = pd.read_csv(data_path)\n",
        "    #df['text'] = df['modified_text']\n",
        "    #df.drop(columns=[\"Unnamed: 0\", \"modified_text\"], inplace=True)\n",
        "    df[\"label\"] = df[\"rating\"] - 1\n",
        "    texts = df[\"text\"].tolist()\n",
        "    labels = np.array(df[\"label\"].tolist())\n",
        "\n",
        "    trainer = GPUOptimizedTrainer(texts, labels)\n",
        "    metrics = trainer.train()\n",
        "\n",
        "    # Plot results\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(metrics['train_loss'], label='Train Loss')\n",
        "    plt.plot(metrics['val_loss'], label='Val Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(metrics['train_acc'], label='Train Acc')\n",
        "    plt.plot(metrics['val_acc'], label='Val Acc')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hTqG67U3EhG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o7lWIilBEhJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HK3uoiDnEhMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A9g4nvc8EhO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cKkqoI0KsYly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6nvZuku0sYoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYcesDXosYrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from model_trainer import GPUOptimizedTrainer, EarlyStopping, RegularizedBERT\n",
        "from reporting_utils import *  # This imports all the reporting functions\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def load_or_compute_embeddings(texts, embedding_save_path, st_model):\n",
        "    \"\"\"Loads precomputed embeddings or computes and saves them if they don't exist.\"\"\"\n",
        "    if os.path.exists(embedding_save_path):\n",
        "        logging.info(f\"Loading embeddings from {embedding_save_path}\")\n",
        "        embeddings = np.load(embedding_save_path)\n",
        "    else:\n",
        "        logging.info(\"Computing embeddings...\")\n",
        "        embeddings = []\n",
        "        for text in tqdm(texts, desc=\"Generating Embeddings\"):\n",
        "            with torch.no_grad():\n",
        "                embedding = st_model.encode(text, convert_to_tensor=True)\n",
        "                embeddings.append(embedding.cpu().numpy())\n",
        "        embeddings = np.array(embeddings)\n",
        "        np.save(embedding_save_path, embeddings)\n",
        "        logging.info(f\"Embeddings saved to {embedding_save_path}\")\n",
        "    return embeddings\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    hyperparams = {\n",
        "        \"Epochs\": 10,\n",
        "        \"Batch Size\": 64,\n",
        "        \"Learning Rate\": 1e-5,\n",
        "        \"Dropout\": 0.3,\n",
        "        \"Weight Decay\": 0.1,\n",
        "        \"Label Smoothing\": 0.1,\n",
        "        \"Early Stopping Patience\": 3,\n",
        "        \"Gradient Accumulation Steps\": 4,\n",
        "        \"Optimizer\": \"AdamW\",\n",
        "        \"Scheduler\": \"OneCycleLR\",\n",
        "        \"Feature Dimension\": 0,  # Will be updated in the code\n",
        "        \"Model\": \"BERT with Multiple Embeddings\"\n",
        "    }\n",
        "    data_path = \"/content/so_many_rev.csv\"  # Change to your data path\n",
        "    df = pd.read_csv(data_path)\n",
        "    df[\"label\"] = df[\"rating\"] - 1\n",
        "    texts = df[\"text\"].tolist()\n",
        "    labels = np.array(df[\"label\"].tolist())\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Initialize Sentence Transformer model\n",
        "    st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Load or compute embeddings\n",
        "    embedding_save_path = \"embeddings.npy\"\n",
        "    contextual_embeddings = load_or_compute_embeddings(texts, embedding_save_path, st_model)\n",
        "\n",
        "    # Pass contextual embeddings to the trainer\n",
        "    trainer = GPUOptimizedTrainer(texts, labels, hyperparams, tokenizer)\n",
        "    metrics, val_loader = trainer.train()\n",
        "\n",
        "    # Generate reports after training\n",
        "    y_true, y_pred, y_probs = get_predictions_and_probabilities(val_loader, trainer.model, trainer.device)\n",
        "    y_probs_all = []  # Store probabilities for all classes\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, features, target in val_loader:\n",
        "            input_ids = input_ids.to(trainer.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(trainer.device, non_blocking=True)\n",
        "            features = features.to(trainer.device, non_blocking=True)\n",
        "            target = target.to(trainer.device, non_blocking=True)\n",
        "\n",
        "            output = trainer.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "            y_probs_all.extend(probabilities.cpu().numpy())  # Store probabilities\n",
        "\n",
        "    # Plot results using Matplotlib\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(metrics['train_loss'], label='Train Loss')\n",
        "    plt.plot(metrics['val_loss'], label='Val Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(metrics['train_acc'], label='Train Acc')\n",
        "    plt.plot(metrics['val_acc'], label='Val Acc')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Generating Reports...\")\n",
        "    logging.info(\"Generating Reports...\")\n",
        "    generate_confusion_matrix(y_true, y_pred)\n",
        "    generate_classification_report(y_true, y_pred)\n",
        "    generate_misclassified_examples(val_loader, trainer.model, tokenizer, trainer.device)\n",
        "    generate_calibration_plot(y_true, y_probs)\n",
        "    generate_embedding_visualization(val_loader, trainer.model, trainer.device)\n",
        "    generate_pr_curves(y_true, y_pred, y_probs_all, trainer.num_classes)\n",
        "    generate_roc_curves(y_true, y_probs_all, trainer.num_classes)\n",
        "    analyze_class_distribution(trainer.labels)\n",
        "    monitor_training_diagnostics(trainer.model, trainer.optimizer, trainer.train_loader, trainer.device)\n",
        "\n",
        "    # Example usage with a sample input text\n",
        "    sample_input_text = \"This is an example input text for attention visualization.\"\n",
        "    visualize_attention_weights(trainer.model, tokenizer, sample_input_text, trainer.device)\n",
        "    perform_shap_analysis(trainer.model, tokenizer, sample_input_text, trainer.device)\n",
        "\n",
        "    # Example: Analyze feature importance\n",
        "    feature_names = [f\"feature_{i}\" for i in range(trainer.feature_dim)]  # Replace with your actual feature names\n",
        "    analyze_feature_importance(trainer.model, trainer.device, feature_names)\n",
        "\n",
        "    # Create a DataFrame for the report data\n",
        "    logging.info(\"Creating report DataFrame...\")\n",
        "    report_data = pd.DataFrame({\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "        'y_probs': list(y_probs_all),\n",
        "        'train_loss': metrics['train_loss'],\n",
        "        'val_loss': metrics['val_loss'],\n",
        "        'train_acc': metrics['train_acc'],\n",
        "        'val_acc': metrics['val_acc'],\n",
        "        'best_val_loss': metrics['best_val_loss'],\n",
        "        # Add other metrics or data you want to include\n",
        "    })\n",
        "\n",
        "    # Write the DataFrame to a CSV file\n",
        "    logging.info(\"Writing report data to CSV...\")\n",
        "    report_data.to_csv(\"model_report.csv\", index=False)\n",
        "    logging.info(\"Report data written to model_report.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "6u_VezbosYtk",
        "outputId": "c01fbafc-3da9-4cae-ee46-09d003feba9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/model_trainer.py:58: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.model = RegularizedBERT(num_labels=5, feature_dim=self.feature_dim, hyperparams=hyperparams).to(self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data...\n",
            "Extracting features...\n",
            "Fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Batches:   0%|          | 0/761 [00:00<?, ?it/s]/content/model_trainer.py:236: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  \n",
            "Training Batches: 100%|██████████| 761/761 [10:33<00:00,  1.20it/s]\n",
            "/content/model_trainer.py:271: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  output = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 1.6197, Acc: 0.2502\n",
            "Val Loss: 1.3981, Acc: 0.4369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Batches: 100%|██████████| 761/761 [10:32<00:00,  1.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "Train Loss: 1.2742, Acc: 0.4955\n",
            "Val Loss: 1.1717, Acc: 0.5667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Batches:   2%|▏         | 19/761 [00:16<10:47,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c1c288022427>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Pass contextual embeddings to the trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUOptimizedTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Generate reports after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entering train function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model_trainer.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting train function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             return {\n\u001b[0;32m--> 244\u001b[0;31m                 \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_trainer.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from torch.cuda.amp import autocast, GradScaler  # Changed to torch.amp\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Initialize spacy model (SentenceTransformer is initialized in main.py)\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'lemmatizer'])\n",
        "\n",
        "class RegularizedBERT(nn.Module):\n",
        "    def __init__(self, num_labels, feature_dim, hyperparams):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(hyperparams[\"Dropout\"])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768 + feature_dim, 384),\n",
        "            nn.LayerNorm(384),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(hyperparams[\"Dropout\"]),\n",
        "            nn.Linear(384, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, features):\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.last_hidden_state[:, 0, :]\n",
        "        combined_features = torch.cat([pooled_output, features], dim=1)\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n",
        "\n",
        "class GPUOptimizedTrainer:\n",
        "    def __init__(self, texts, labels, hyperparams, tokenizer, contextual_embeddings):\n",
        "      try:\n",
        "          self.device = torch.device('cuda')\n",
        "          self.hyperparams = hyperparams\n",
        "          self.batch_size = hyperparams[\"Batch Size\"]\n",
        "          self.tokenizer = tokenizer\n",
        "          self.contextual_embeddings = contextual_embeddings\n",
        "\n",
        "          torch.backends.cudnn.benchmark = True\n",
        "          torch.backends.cuda.matmul.allow_tf32 = True\n",
        "          torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "          self.feature_dim = self.extract_features(texts[:1]).shape[1]\n",
        "          self.hyperparams[\"Feature Dimension\"] = self.feature_dim\n",
        "\n",
        "          self.model = RegularizedBERT(num_labels=5, feature_dim=self.feature_dim, hyperparams=hyperparams).to(self.device)\n",
        "\n",
        "          self.scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "          self.labels = labels\n",
        "          self.prepare_data(texts)\n",
        "          self.setup_training()\n",
        "      except Exception as e:\n",
        "          logging.error(f\"An error occurred in GPUOptimizedTrainer.__init__: {e}\")\n",
        "          traceback.print_exc()\n",
        "          raise\n",
        "\n",
        "    def extract_features(self, texts):\n",
        "      logging.info(\"Entering extract_features function\")\n",
        "      try:\n",
        "          print(\"Extracting features...\")\n",
        "\n",
        "          def extract_syntactic(texts):\n",
        "              features = []\n",
        "              for text in tqdm(texts, desc=\"Extracting Syntactic Features\"):\n",
        "                  doc = nlp(text)\n",
        "                  pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "                  features.append([\n",
        "                      len(doc),\n",
        "                      len(set(pos_tags)) / len(pos_tags),\n",
        "                      pos_tags.count('NOUN') / len(pos_tags),\n",
        "                      pos_tags.count('VERB') / len(pos_tags),\n",
        "                  ])\n",
        "              return np.array(features)\n",
        "\n",
        "          # Use precomputed contextual embeddings\n",
        "          if self.contextual_embeddings is not None:\n",
        "              contextual_features = self.contextual_embeddings[:len(texts)]\n",
        "          else:\n",
        "              contextual_features = []\n",
        "\n",
        "          syntactic_features = extract_syntactic(texts)\n",
        "          logging.info(\"Exiting extract_features function\")\n",
        "          return np.hstack([contextual_features, syntactic_features])\n",
        "      except Exception as e:\n",
        "          logging.error(f\"An error occurred in extract_features: {e}\")\n",
        "          traceback.print_exc()\n",
        "          raise\n",
        "\n",
        "    def prepare_data(self, texts):\n",
        "      logging.info(\"Entering prepare_data function\")\n",
        "      try:\n",
        "          print(\"Preparing data...\")\n",
        "\n",
        "          # Tokenize text using the tokenizer\n",
        "          encodings = self.tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
        "          input_ids = encodings['input_ids']\n",
        "          attention_mask = encodings['attention_mask']\n",
        "\n",
        "          # Extract features\n",
        "          features = self.extract_features(texts)\n",
        "\n",
        "          # Scale features\n",
        "          scaler = StandardScaler()\n",
        "          features = scaler.fit_transform(features)\n",
        "\n",
        "          # Convert to tensors\n",
        "          self.input_ids = input_ids\n",
        "          self.attention_mask = attention_mask\n",
        "          self.features = torch.tensor(features, dtype=torch.float32)\n",
        "          self.num_classes = len(np.unique(self.labels))\n",
        "          self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
        "\n",
        "          # Create data loaders\n",
        "          self.create_dataloaders()\n",
        "          logging.info(\"Exiting prepare_data function\")\n",
        "      except Exception as e:\n",
        "          logging.error(f\"An error occurred in prepare_data: {e}\")\n",
        "          traceback.print_exc()\n",
        "          raise\n",
        "\n",
        "    def create_dataloaders(self):\n",
        "        logging.info(\"Entering create_dataloaders function\")\n",
        "        try:\n",
        "            dataset = TensorDataset(self.input_ids, self.attention_mask, self.features, self.labels)\n",
        "\n",
        "            self.train_loader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True,\n",
        "                pin_memory=True,\n",
        "                num_workers=4,\n",
        "                prefetch_factor=3,\n",
        "                persistent_workers=True\n",
        "            )\n",
        "            logging.info(\"Exiting create_dataloaders function\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred in create_dataloaders: {e}\")\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "    def setup_training(self):\n",
        "        logging.info(\"Entering setup_training function\")\n",
        "        try:\n",
        "            if self.hyperparams[\"Optimizer\"] == \"AdamW\":\n",
        "                self.optimizer = torch.optim.AdamW(\n",
        "                    self.model.parameters(),\n",
        "                    lr=self.hyperparams[\"Learning Rate\"],\n",
        "                    weight_decay=self.hyperparams[\"Weight Decay\"],\n",
        "                    betas=(0.9, 0.999)\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Optimizer {self.hyperparams['Optimizer']} not supported\")\n",
        "\n",
        "            if self.hyperparams[\"Scheduler\"] == \"OneCycleLR\":\n",
        "                self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "                    self.optimizer,\n",
        "                    max_lr=self.hyperparams[\"Learning Rate\"] * 2,\n",
        "                    epochs=self.hyperparams[\"Epochs\"],\n",
        "                    steps_per_epoch=len(self.train_loader) // self.hyperparams[\"Gradient Accumulation Steps\"],\n",
        "                    pct_start=0.1\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Scheduler {self.hyperparams['Scheduler']} not supported\")\n",
        "\n",
        "            self.early_stopping = EarlyStopping(patience=self.hyperparams[\"Early Stopping Patience\"])\n",
        "            logging.info(\"Exiting setup_training function\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred in setup_training: {e}\")\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "    def train(self):\n",
        "        logging.info(\"Entering train function\")\n",
        "        try:\n",
        "            best_val_loss = float('inf')\n",
        "            train_losses, val_losses = [], []\n",
        "            train_accs, val_accs = [], []\n",
        "\n",
        "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "            for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(self.input_ids, self.labels), total=skf.get_n_splits(), desc=\"Folds\")):\n",
        "                logging.info(f\"Starting fold {fold + 1}\")\n",
        "                print(f\"Fold {fold + 1}\")\n",
        "\n",
        "                train_dataset = TensorDataset(\n",
        "                    self.input_ids[train_idx],\n",
        "                    self.attention_mask[train_idx],\n",
        "                    self.features[train_idx],\n",
        "                    self.labels[train_idx]\n",
        "                )\n",
        "                val_dataset = TensorDataset(\n",
        "                    self.input_ids[val_idx],\n",
        "                    self.attention_mask[val_idx],\n",
        "                    self.features[val_idx],\n",
        "                    self.labels[val_idx]\n",
        "                )\n",
        "\n",
        "                train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, pin_memory=True, num_workers=4, prefetch_factor=3, persistent_workers=True)\n",
        "                val_loader = DataLoader(val_dataset, batch_size=self.batch_size * 2, pin_memory=True, num_workers=4)\n",
        "\n",
        "                for epoch in range(self.hyperparams[\"Epochs\"]):\n",
        "                    logging.info(f\"Starting epoch {epoch + 1}\")\n",
        "                    train_metrics = self.train_epoch(train_loader)\n",
        "                    val_metrics = self.validate(val_loader)\n",
        "\n",
        "                    train_losses.append(train_metrics['loss'])\n",
        "                    val_losses.append(val_metrics['loss'])\n",
        "                    train_accs.append(train_metrics['acc'])\n",
        "                    val_accs.append(val_metrics['acc'])\n",
        "\n",
        "                    print(f\"Epoch {epoch+1}/{self.hyperparams['Epochs']}\")\n",
        "                    print(f\"Train Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['acc']:.4f}\")\n",
        "                    print(f\"Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
        "\n",
        "                    if val_metrics['loss'] < best_val_loss:\n",
        "                        best_val_loss = val_metrics['loss']\n",
        "                        torch.save(self.model.state_dict(), 'best_model.pt')\n",
        "\n",
        "                    if self.early_stopping(val_metrics['loss']):\n",
        "                        logging.info(\"Early stopping triggered\")\n",
        "                        print(\"Early stopping triggered\")\n",
        "                        break\n",
        "\n",
        "                    self.scheduler.step()\n",
        "                    logging.info(f\"Finished epoch {epoch + 1}\")\n",
        "\n",
        "                logging.info(f\"Finished fold {fold + 1}\")\n",
        "\n",
        "            logging.info(\"Exiting train function\")\n",
        "            return {\n",
        "                'train_loss': train_losses,\n",
        "                'val_loss': val_losses,\n",
        "                'train_acc': train_accs,\n",
        "                'val_acc': val_accs,\n",
        "                'best_val_loss': best_val_loss\n",
        "            }, val_loader  # Return val_loader\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred in train: {e}\")\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        logging.info(\"Entering train_epoch function\")\n",
        "        try:\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            for batch_idx, (input_ids, attention_mask, features, target) in enumerate(tqdm(train_loader, desc=\"Training Batches\")):\n",
        "                input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "                attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "                features = features.to(self.device, non_blocking=True)\n",
        "                target = target.to(self.device, non_blocking=True)\n",
        "\n",
        "                with autocast():\n",
        "                    output = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                    loss = self.label_smoothed_nll_loss(output, target) / self.hyperparams[\"Gradient Accumulation Steps\"]\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "\n",
        "                if (batch_idx + 1) % self.hyperparams[\"Gradient Accumulation Steps\"] == 0 or (batch_idx + 1) == len(train_loader):\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                    self.scaler.step(self.optimizer)\n",
        "                    self.scaler.update()\n",
        "                    self.optimizer.zero_grad(set_to_none=True)\n",
        "                    self.scheduler.step()\n",
        "\n",
        "                total_loss += loss.item() * self.hyperparams[\"Gradient Accumulation Steps\"]\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "            logging.info(\"Exiting train_epoch function\")\n",
        "            return {'loss': total_loss / len(train_loader), 'acc': correct / total}\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred in train_epoch: {e}\")\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        logging.info(\"Entering validate function\")\n",
        "        try:\n",
        "            self.model.eval()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for batch_idx, (input_ids, attention_mask, features, target) in enumerate(tqdm(val_loader, desc=\"Processing Validation Batches\")):\n",
        "                input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "                attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "                features = features.to(self.device, non_blocking=True)\n",
        "                target = target.to(self.device, non_blocking=True)\n",
        "\n",
        "                with autocast():\n",
        "                    output = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                    loss = self.label_smoothed_nll_loss(output, target)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "            logging.info(\"Exiting validate function\")\n",
        "            return {'loss': total_loss / len(val_loader), 'acc': correct / total}\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred in validate: {e}\")\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "    def label_smoothed_nll_loss(self, logits, target):\n",
        "        try:\n",
        "            epsilon = self.hyperparams[\"Label Smoothing\"]\n",
        "            num_classes = logits.size(-1)\n",
        "\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            one_hot = torch.zeros_like(log_probs).scatter(1, target.unsqueeze(-1), 1)\n",
        "            one_hot = one_hot * (1 - epsilon) + (1 - one_hot) * epsilon / (num_classes - 1)\n",
        "            loss = -(one_hot * log_probs).sum(dim=-1)\n",
        "\n",
        "            return loss.mean()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred in label_smoothed_nll_loss: {e}\")\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        return False"
      ],
      "metadata": {
        "id": "b7wavDu2sYwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reporting_utils.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "import time\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import shap\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict, Optional, Union\n",
        "import traceback\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def generate_confusion_matrix(y_true: List[int], y_pred: List[int], normalize: Optional[str] = 'true') -> None:\n",
        "    \"\"\"Generates and displays a confusion matrix.\"\"\"\n",
        "    logging.info(\"Generating confusion matrix...\")\n",
        "    try:\n",
        "        cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt=\".2f\" if normalize else \"d\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"True\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "        print(cm)\n",
        "        logging.info(\"Confusion matrix generated.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in generate_confusion_matrix: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def generate_classification_report(y_true: List[int], y_pred: List[int]) -> None:\n",
        "    \"\"\"Generates and prints a classification report.\"\"\"\n",
        "    logging.info(\"Generating classification report...\")\n",
        "    try:\n",
        "        print(classification_report(y_true, y_pred))\n",
        "        logging.info(\"Classification report generated.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in generate_classification_report: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def generate_misclassified_examples(val_loader: torch.utils.data.DataLoader, model: torch.nn.Module, tokenizer, device: torch.device) -> None:\n",
        "    \"\"\"Generates and prints misclassified examples.\"\"\"\n",
        "    logging.info(\"Generating misclassified examples...\")\n",
        "    try:\n",
        "        model.eval()\n",
        "        misclassified_examples: List[Tuple[str, int, int, np.ndarray]] = []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, features, target in tqdm(val_loader, desc=\"Processing batches\"):\n",
        "                input_ids = input_ids.to(device, non_blocking=True)\n",
        "                attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "                features = features.to(device, non_blocking=True)\n",
        "                target = target.to(device, non_blocking=True)\n",
        "\n",
        "                output = model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                pred = output.argmax(dim=1)\n",
        "\n",
        "                for i in range(len(target)):\n",
        "                    if pred[i] != target[i]:\n",
        "                        text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
        "                        misclassified_examples.append((text, target[i].item(), pred[i].item(), output[i].cpu().numpy()))\n",
        "\n",
        "        for text, true_label, pred_label, _ in misclassified_examples:\n",
        "            print(f\"Text: {text}\\nTrue Label: {true_label}, Predicted Label: {pred_label}\\n---\")\n",
        "        logging.info(\"Misclassified examples generated.\")\n",
        "\n",
        "        # Confidence distribution analysis\n",
        "        logging.info(\"Analyzing confidence distribution...\")\n",
        "        correct_confidences: List[float] = []\n",
        "        incorrect_confidences: List[float] = []\n",
        "        for text, true_label, pred_label, output in misclassified_examples:\n",
        "            probs = F.softmax(torch.tensor(output), dim=0)\n",
        "            if true_label == pred_label:\n",
        "                correct_confidences.append(probs[pred_label].item())\n",
        "            else:\n",
        "                incorrect_confidences.append(probs[pred_label].item())\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(correct_confidences, label=\"Correct Predictions\", kde=True)\n",
        "        sns.histplot(incorrect_confidences, label=\"Incorrect Predictions\", kde=True)\n",
        "        plt.xlabel(\"Confidence\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.title(\"Confidence Distribution for Correct vs Incorrect Predictions\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        logging.info(\"Confidence distribution analysis complete.\")\n",
        "\n",
        "        # Most confident mistakes\n",
        "        logging.info(\"Identifying most confident mistakes...\")\n",
        "        incorrect_examples = [(text, true_label, pred_label, output) for text, true_label, pred_label, output in misclassified_examples if true_label != pred_label]\n",
        "        incorrect_examples.sort(key=lambda x: F.softmax(torch.tensor(x[3]), dim=0)[x[2]].item(), reverse=True)\n",
        "\n",
        "        print(\"\\nTop 5 Most Confident Mistakes:\")\n",
        "        for text, true_label, pred_label, _ in incorrect_examples[:5]:\n",
        "            print(f\"Text: {text}\\nTrue Label: {true_label}, Predicted Label: {pred_label}\\n---\")\n",
        "        logging.info(\"Most confident mistakes identified.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in generate_misclassified_examples: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def generate_calibration_plot(y_true: Union[List[int], np.ndarray], y_probs: Union[List[float], np.ndarray], n_bins: int = 10) -> None:\n",
        "    \"\"\"Generates and displays a calibration plot.\"\"\"\n",
        "    logging.info(\"Generating calibration plot...\")\n",
        "    try:\n",
        "        fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_probs, n_bins=n_bins)\n",
        "\n",
        "        # Calibration (using Isotonic Regression)\n",
        "        ir = IsotonicRegression(out_of_bounds='clip')\n",
        "        y_probs_calibrated = ir.fit_transform(y_probs, y_true)\n",
        "        fraction_of_positives_calibrated, mean_predicted_value_calibrated = calibration_curve(y_true, y_probs_calibrated, n_bins=n_bins)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Uncalibrated\")\n",
        "        plt.plot(mean_predicted_value_calibrated, fraction_of_positives_calibrated, \"s-\", label=\"Calibrated (Isotonic)\")\n",
        "        plt.plot([0, 1], [0, 1], \"--\", color=\"gray\", label=\"Perfectly Calibrated\")\n",
        "        plt.xlabel(\"Mean Predicted Probability\")\n",
        "        plt.ylabel(\"Fraction of Positives\")\n",
        "        plt.legend()\n",
        "        plt.title(\"Calibration Plot\")\n",
        "        plt.show()\n",
        "        logging.info(\"Calibration plot generated.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in generate_calibration_plot: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def generate_embedding_visualization(val_loader: torch.utils.data.DataLoader, model: torch.nn.Module, device: torch.device, n_components: int = 2) -> None:\n",
        "    \"\"\"Generates and displays a t-SNE visualization of embeddings.\"\"\"\n",
        "    logging.info(\"Generating embedding visualization...\")\n",
        "    try:\n",
        "        model.eval()\n",
        "        embeddings: List[np.ndarray] = []\n",
        "        y_true: List[int] = []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, features, target in tqdm(val_loader, desc=\"Processing batches\"):\n",
        "                input_ids = input_ids.to(device, non_blocking=True)\n",
        "                attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "                features = features.to(device, non_blocking=True)\n",
        "\n",
        "                bert_output = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                pooled_output = bert_output.last_hidden_state[:, 0, :]\n",
        "                embeddings.extend(pooled_output.cpu().numpy())\n",
        "                y_true.extend(target.cpu().numpy())\n",
        "\n",
        "        embeddings_array: np.ndarray = np.array(embeddings)\n",
        "        tsne = TSNE(n_components=n_components, random_state=42, perplexity=30, n_iter=300)\n",
        "        embeddings_2d: np.ndarray = tsne.fit_transform(embeddings_array)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        unique_labels = np.unique(y_true)\n",
        "        for i in range(len(unique_labels)):\n",
        "            plt.scatter(embeddings_2d[np.array(y_true) == i, 0], embeddings_2d[np.array(y_true) == i, 1], label=f\"Class {i}\")\n",
        "        plt.legend()\n",
        "        plt.title(\"t-SNE Visualization of Embeddings\")\n",
        "        plt.show()\n",
        "        logging.info(\"Embedding visualization generated.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in generate_embedding_visualization: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def get_predictions_and_probabilities(val_loader: torch.utils.data.DataLoader, model: torch.nn.Module, device: torch.device) -> Tuple[List[int], List[int], List[np.ndarray]]:\n",
        "    \"\"\"Gets predictions, true labels, and probabilities from the validation set.\"\"\"\n",
        "    logging.info(\"Getting predictions and probabilities...\")\n",
        "    try:\n",
        "        model.eval()\n",
        "        y_true: List[int] = []\n",
        "        y_pred: List[int] = []\n",
        "        y_probs: List[np.ndarray] = []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, features, target in tqdm(val_loader, desc=\"Processing batches\"):\n",
        "                input_ids = input_ids.to(device, non_blocking=True)\n",
        "                attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "                features = features.to(device, non_blocking=True)\n",
        "                target = target.to(device, non_blocking=True)\n",
        "\n",
        "                output = model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                probabilities = F.softmax(output, dim=1)\n",
        "                pred = output.argmax(dim=1)\n",
        "\n",
        "                y_true.extend(target.cpu().numpy())\n",
        "                y_pred.extend(pred.cpu().numpy())\n",
        "                y_probs.extend(probabilities.cpu().numpy())  # Store probabilities directly\n",
        "        logging.info(\"Predictions and probabilities obtained.\")\n",
        "        return y_true, y_pred, y_probs\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in get_predictions_and_probabilities: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def generate_pr_curves(y_true: List[int], y_pred: List[int], y_probs: List[np.ndarray], num_classes: int) -> None:\n",
        "    \"\"\"Generates precision-recall curves for each class.\"\"\"\n",
        "    logging.info(\"Generating precision-recall curves...\")\n",
        "    try:\n",
        "        y_true_bin = np.zeros((len(y_true), num_classes))\n",
        "        for i, label in enumerate(y_true):\n",
        "            y_true_bin[i, label] = 1\n",
        "\n",
        "        y_probs_array: np.ndarray = np.array(y_probs)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        for i in range(num_classes):\n",
        "            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_probs_array[:, i])\n",
        "            plt.plot(recall, precision, label=f'Class {i}')\n",
        "\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curves for Each Class')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        logging.info(\"Precision-recall curves generated.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in generate_pr_curves: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def generate_roc_curves(y_true: List[int], y_probs: List[np.ndarray], num_classes: int) -> None:\n",
        "    \"\"\"Generates ROC curves for each class.\"\"\"\n",
        "    logging.info(\"Generating ROC curves...\")\n",
        "    try:\n",
        "        y_true_bin = np.zeros((len(y_true), num_classes))\n",
        "        for i, label in enumerate(y_true):\n",
        "            y_true_bin[i, label] = 1\n",
        "\n",
        "        y_probs_array: np.ndarray = np.array(y_probs)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        for i in range(num_classes):\n",
        "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_probs_array[:, i])\n",
        "            plt.plot(fpr, tpr, label=f'Class {i}')\n",
        "\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curves for Each Class')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        logging.info(\"ROC curves generated.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in generate_roc_curves: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def analyze_class_distribution(labels: Union[List[int], np.ndarray]) -> None:\n",
        "    \"\"\"Analyzes the distribution of classes in the dataset.\"\"\"\n",
        "    logging.info(\"Analyzing class distribution...\")\n",
        "    try:\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        distribution: Dict[int, int] = dict(zip(unique, counts))\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.bar(distribution.keys(), distribution.values())\n",
        "        plt.xlabel(\"Class\")\n",
        "        plt.ylabel(\"Number of Samples\")\n",
        "        plt.title(\"Class Distribution\")\n",
        "        plt.show()\n",
        "\n",
        "        logging.info(\"Class distribution analysis complete.\")\n",
        "        print(\"Class Distribution:\", distribution)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in analyze_class_distribution: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def monitor_training_diagnostics(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_loader: torch.utils.data.DataLoader, device: torch.device) -> None:\n",
        "    \"\"\"Monitors gradient norms, activation statistics, and learning rate changes.\"\"\"\n",
        "    logging.info(\"Monitoring training diagnostics...\")\n",
        "    try:\n",
        "        # Gradient Norms\n",
        "        gradient_norms: List[float] = []\n",
        "        for batch_idx, (input_ids, attention_mask, features, target) in enumerate(tqdm(train_loader, desc=\"Training Batches\")):\n",
        "            input_ids = input_ids.to(device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "            features = features.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            total_norm = 0\n",
        "            for p in model.parameters():\n",
        "                param_norm = p.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "            total_norm = total_norm ** 0.5\n",
        "            gradient_norms.append(total_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        # Layer-wise Activation Statistics\n",
        "        activation_stats: Dict[str, List[np.ndarray]] = {}\n",
        "        def get_activation_stats(name: str) -> Callable:\n",
        "            def hook(model: torch.nn.Module, input: Tuple, output: torch.Tensor) -> None:\n",
        "                if isinstance(output, torch.Tensor):\n",
        "                    activation_stats.setdefault(name, []).append(output.detach().cpu().numpy())\n",
        "            return hook\n",
        "\n",
        "        for name, layer in model.named_modules():\n",
        "            if isinstance(layer, torch.nn.Linear):  # You can add more layer types here\n",
        "                layer.register_forward_hook(get_activation_stats(name))\n",
        "\n",
        "        # Pass data through the model to collect activations\n",
        "        with torch.no_grad():\n",
        "          for input_ids, attention_mask, features, target in tqdm(train_loader, desc=\"Processing Batches for Activation Stats\"):\n",
        "              input_ids = input_ids.to(device, non_blocking=True)\n",
        "              attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "              features = features.to(device, non_blocking=True)\n",
        "              target = target.to(device, non_blocking=True)\n",
        "              model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "\n",
        "        # Learning Rate Changes\n",
        "        lrs: List[float] = []\n",
        "        for param_group in optimizer.param_groups:\n",
        "            lrs.append(param_group['lr'])\n",
        "\n",
        "        # Memory Usage\n",
        "        memory_usage = torch.cuda.memory_allocated(device) / (1024 ** 3)  # in GB\n",
        "        logging.info(f\"Memory usage: {memory_usage:.2f} GB\")\n",
        "\n",
        "        # Plotting\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(gradient_norms)\n",
        "        plt.xlabel(\"Batch\")\n",
        "        plt.ylabel(\"Gradient Norm\")\n",
        "        plt.title(\"Gradient Norms During Training\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        for layer_name, activations in activation_stats.items():\n",
        "          all_activations = np.concatenate(activations).flatten()\n",
        "          sns.histplot(all_activations, kde=True, label=layer_name)\n",
        "        plt.xlabel(\"Activation Value\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.title(\"Layer-wise Activation Statistics\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(lrs)\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Learning Rate\")\n",
        "        plt.title(\"Learning Rate Changes\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Cleanup\n",
        "        del activation_stats\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        logging.info(\"Training diagnostics monitoring complete.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in monitor_training_diagnostics: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def visualize_attention_weights(model: torch.nn.Module, tokenizer, input_text: str, device: torch.device) -> None:\n",
        "    \"\"\"Visualizes attention weights for a given input text.\"\"\"\n",
        "    logging.info(f\"Visualizing attention weights for input: '{input_text}'\")\n",
        "    try:\n",
        "        model.eval()\n",
        "\n",
        "        # Tokenize the input text\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.no_grad():\n",
        "            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
        "            attentions = outputs.attentions\n",
        "\n",
        "        # For simplicity, let's focus on the attention from the last layer\n",
        "        # We'll take the average attention across all heads\n",
        "        last_layer_attention = attentions[-1]  # Shape: (batch_size, num_heads, sequence_length, sequence_length)\n",
        "        avg_attention = torch.mean(last_layer_attention, dim=1).squeeze(0)  # Shape: (sequence_length, sequence_length)\n",
        "\n",
        "        # Convert token IDs back to tokens\n",
        "        tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
        "\n",
        "        # Create a heatmap of the attention weights\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(avg_attention.cpu().numpy(), xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n",
        "        plt.title(\"Attention Weights Heatmap\")\n",
        "        plt.xlabel(\"Target Tokens\")\n",
        "        plt.ylabel(\"Source Tokens\")\n",
        "        plt.show()\n",
        "        logging.info(\"Attention weights visualization generated.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in visualize_attention_weights: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def perform_shap_analysis(model: torch.nn.Module, tokenizer, input_text: str, device: torch.device) -> None:\n",
        "    \"\"\"Performs SHAP analysis for a given input text.\"\"\"\n",
        "    logging.info(f\"Performing SHAP analysis for input: '{input_text}'\")\n",
        "    try:\n",
        "        model.eval()\n",
        "\n",
        "        # Tokenize the input text\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Define a prediction function for SHAP\n",
        "        def predict_fn(texts: np.ndarray) -> np.ndarray:\n",
        "            inputs = tokenizer(texts.tolist(), return_tensors=\"pt\", truncation=True, padding=True)\n",
        "            input_ids = inputs[\"input_ids\"].to(device)\n",
        "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Assuming you have a way to create 'features' for these texts\n",
        "                # If not, you might need to adjust this part\n",
        "                features = torch.zeros((len(texts), 388), dtype=torch.float32).to(device)  # Example placeholder\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "            return probs.cpu().numpy()\n",
        "\n",
        "        # Create a SHAP explainer\n",
        "        explainer = shap.Explainer(predict_fn, tokenizer)\n",
        "\n",
        "        # Calculate SHAP values\n",
        "        shap_values = explainer([input_text])\n",
        "\n",
        "        # Visualize the SHAP values for the first prediction\n",
        "        shap.plots.text(shap_values[0])\n",
        "        logging.info(\"SHAP analysis complete.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in perform_shap_analysis: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def analyze_feature_importance(model: torch.nn.Module, device: torch.device, feature_names: List[str]) -> None:\n",
        "    \"\"\"Analyzes feature importance if applicable (e.g., for linear models).\"\"\"\n",
        "    logging.info(\"Analyzing feature importance...\")\n",
        "    try:\n",
        "        # Check if the model has a linear layer\n",
        "        linear_layer = None\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                linear_layer = module\n",
        "                break\n",
        "\n",
        "        if linear_layer is None:\n",
        "            logging.warning(\"Feature importance analysis not applicable for this model type.\")\n",
        "            return\n",
        "\n",
        "        # Get the weights of the linear layer\n",
        "        weights = linear_layer.weight.data.cpu().numpy()\n",
        "\n",
        "        # For simplicity, let's consider the absolute average weight for each feature\n",
        "        avg_weights = np.abs(weights).mean(axis=0)\n",
        "\n",
        "        # Create a DataFrame for better visualization\n",
        "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': avg_weights})\n",
        "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "        # Plot feature importance\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='Importance', y='Feature', data=importance_df, orient='h')\n",
        "        plt.title('Feature Importance')\n",
        "        plt.xlabel('Average Absolute Weight')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.show()\n",
        "\n",
        "        logging.info(\"Feature importance analysis complete.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in analyze_feature_importance: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise"
      ],
      "metadata": {
        "id": "r_7JREhbsYyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7WYLCLZLsY1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkHiSkJssY3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yj7uvifXsY5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qv8d_jKosY7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-HkI8ndso5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}